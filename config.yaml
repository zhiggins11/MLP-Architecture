# This specifies the number of layers and number of hidden neurons in each layer.
# Note that the first and the last elements of the list indicate the input and
# output sizes
layer_specs: [784, 50, 50, 10]

# Type of non-linear activation function to be used for the layers.  Options are "tanh", "ReLU", "leakyReLU", and "sigmoid"
activation: "ReLU"

# The learning rate to be used for training.
learning_rate: 0.001

# Number of training samples per batch to be passed to network
batch_size: 128

# Number of epochs to train the model
epochs: 15

# Check the validation loss every early_stop_epoch number of epochs to find the best weights.
early_stop_epoch: 1

# Regularization constant
L2_penalty: 0

# Use momentum for training
momentum: False

# Value for the parameter 'gamma' in momentum
momentum_gamma: 0.9
